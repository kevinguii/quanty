{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34619944",
   "metadata": {},
   "source": [
    "# 9/10/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd7aabf",
   "metadata": {},
   "source": [
    "## Monte Carlo\n",
    "\n",
    "By introducing randomness, Monte Carlo simulation helps us integrate high-dimensional functions exponentially faster than traditional algorithms.\n",
    "\n",
    "Example: integrating over sqrt(x) by splitting into infintely many small interval lengths and summing areas, more will get a closer answer with lower error\n",
    "- can also partition into trapezoids, etc\n",
    "- this is quadrature\n",
    "\n",
    "However, quadrature if hard for high-dimensional, as you would need to exponentially increase number of samples for inc dimensions.\n",
    "\n",
    "For approximating area of a circle, we can use uniformly random samples and average it. It gets us a good approximation of pi. \n",
    "\n",
    "Monte Carlo works because its expected value is equal to the desired integral. The estimator will converge to the correct answer as we increase number of samples. \n",
    "\n",
    "It is better than quadrature also because its expected error decreases at an accelerated rate. As we increase dimensionality, it becomes exponentially quicker than quadrature.\n",
    "\n",
    "Bias: difference between expected value and desired integral\n",
    "- unbiased if expected value is exactly the correct answer\n",
    "- Monte Carlo is unbiased\n",
    "\n",
    "Quadrature is consistent, won't give different values, Monte Carlow will."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18375e9d",
   "metadata": {},
   "source": [
    "## Random Variables Review\n",
    "\n",
    "Discrete: countable number of outcomes\n",
    "Continuous: infinitely many outcomes, usually in a range\n",
    "\n",
    "PMF (Probability Mass Function): \n",
    "- for discrete random variables, records the chance of seeing each outcome\n",
    "- should sum up to 1 across all outcomes\n",
    "\n",
    "PDF (Probability Density Function):\n",
    "- for continuous random variables, integrate over entire domain should be 1\n",
    "- value of pdf at any given value should be 0\n",
    "\n",
    "CDF (Cumulative Density Function):\n",
    "- probability that RV falls within region\n",
    "- equated by integrating the PDF\n",
    "\n",
    "- for discrete, should be the sum of PMF up to a given point\n",
    "\n",
    "Joint Distribution:\n",
    "- multivariable, two-dimensional, summing must still be equal to 1 for discrete and integration over domain\n",
    "\n",
    "Dependent Variables in Joint Dist:\n",
    "- use conditional distribution of each variable\n",
    "\n",
    "Expectation:\n",
    "- typical result of sampling X, mean of X\n",
    "- sum outcomes of X weighted by their probabilities\n",
    "- in discrete world, use sum, in continuous, use integrals\n",
    "- expectation is linear: E[X+Y] = E[X] + E[Y]\n",
    "\n",
    "Dispersion:\n",
    "- average distance a sample will fall from the mean\n",
    "- high dispersion means we expect more samples away from the Expected Value\n",
    "- quantitative measure is variance: E[X-E[X]^2] = E[X^2] - (E[X])^2\n",
    "\n",
    "Markov's Inequality:\n",
    "- for any-nonnegative X (RV), probability of drawing X >=a is at most E[X]/a\n",
    "\n",
    "Standard Deviation:\n",
    "- square root of variance, more intuitive measure of dispersion\n",
    "- most relevant in normal distribution\n",
    "\n",
    "Covariance: \n",
    "- measures how strongly two variables are correlated\n",
    "- = E[XY] - E[X]E[Y]\n",
    "- independent means no covariance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64bf34e",
   "metadata": {},
   "source": [
    "## Tail Dependence\n",
    "This measures how two variables tend to move together during extreme events, focusing on the tails of the joint distribution, where extreme events likely occur. It quantifies the likelihood that two variables will simultaneously experience extreme high values (upper tail dependence) or extreme low values (lower tail dependence).\n",
    "\n",
    "Basically it quantifies how likely multiple extreme events are correlated to each other. Example is when Russia defaulted on its debt in 1998, all global markets plunged, therefore high leverage goes belly up."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
